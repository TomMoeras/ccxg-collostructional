write.table(assoc.cor, file=file.path(project_dir, "results_analysis", str_glue("assoc_cor_{basename(file_output)}.csv")), sep="\t", row.names=TRUE, col.names=NA, quote=FALSE)
# corrgram(assoc, order = TRUE, lower.panel = panel.shade, upper.panel = panel.pie)
# corrgram(assoc, order = TRUE, lower.panel = panel.ellipse, upper.panel = panel.pts)
}
file_input = file.path(project_dir, "data", "corpus_cleaned", "cxn_lemma_distinctive.csv")
dist.collexemes()
file_input = file.path(project_dir, "data", "corpus_cleaned", "cxn_roleset_distinctive.csv")
dist.collexemes()
dist.collexemes <- function(precbitsexponent = precbitsexponent) {
# Suppress warning messages
options(warn = -1)
# Initialize variables
dists <- 1
input.dc <- 1
fye.mpfr <- "no" # change input to "yes" if you want the FYE calculated as well. Warning: this takes a long time!
file_input <- file.choose()
# Get the base name of the file (without the file extension)
file_input_base <- basename(file_input)
# Create the filepath for the output file
file_output <- file.path(project_dir, "results_analysis", "reproduced_analyses", file_input_base)
file_output <- paste0(gsub("\\..*", "", file_output), "_reproduced_analysis.csv")
# Read the input file as a table
input.matrix <- read.table(file_input, header = TRUE, sep = "\t", quote = "", comment.char = "")
if (input.dc == 1) { # DCA
# Transpose the table
interim <- t(table(input.matrix))
# Convert the table to a dataframe
input.matrix <- data.frame(
as.vector(interim[,1]),
as.vector(interim[,2]), row.names = rownames(interim))
# Rename the columns
colnames(input.matrix) <- colnames(interim)
} else {
temp <- colnames(input.matrix)[2:3]
# Convert the table to a dataframe
input.matrix <- data.frame(
as.vector(input.matrix[,2]),
as.vector(input.matrix[,3]), row.names = input.matrix[,1])
# Rename the columns
colnames(input.matrix) <- temp
}
# Get the name of the first construction
construction1.name <- colnames(input.matrix)[1]; construction2.name <- colnames(input.matrix)[2]
# computation
options(warn=-1)
# calculate Pearson's residuals for each word-construction pair
pearson.residuals <- chisq.test(input.matrix, correct=FALSE)$residuals[,1]
# create a list of 2x2 matrices for each word-construction pair
all.2.by.2.matrices <- apply(
input.matrix, 1,
\(af) { matrix(c(af, colSums(input.matrix)-af), byrow=TRUE, ncol=2) },
simplify=FALSE)
# if FYE calculation is desired, calculate FYE values for each 2x2 matrix
if (fye.mpfr=="yes") {
FYE.values <- lapply(all.2.by.2.matrices,
\(af) fisher.test.mpfr(af))
}
# calculate log-odds ratios for each 2x2 matrix
glms <- lapply(all.2.by.2.matrices,
\(af) glm(rbind(af[1,], af[2,]) ~ c(1:2), family=binomial))
log.odds.ratios <- -sapply(glms, coefficients)[2,]
# calculate log-likelihood values for each 2x2 matrix
log.likelihood.values <- sapply(glms, "[[", "null.deviance")
# calculate MI scores for each 2x2 matrix
mi.scores <- sapply(all.2.by.2.matrices,
\(af) log2(af[1,1] / chisq.test(af, correct=FALSE)$exp[1,1]))
# calculate delta p values for each 2x2 matrix
delta.p.constr.cues.word <- sapply(all.2.by.2.matrices,
\(af) af[1,1]/sum(af[,1]) - af[1,2]/sum(af[,2]))
delta.p.word.cues.constr <- sapply(all.2.by.2.matrices,
\(af) af[1,1]/sum(af[1,]) - af[2,1]/sum(af[2,]))
# determine whether each word-construction pair is associated or not
relations <- sapply(pearson.residuals,
\(af) switch(sign(af)+2, construction2.name, "chance", construction1.name))
# output
output.table <- data.frame(WORD=rownames(input.matrix), CONSTRUCTION1=input.matrix[,1], CONSTRUCTION2=input.matrix[,2], row.names=NULL)
# This line creates a dataframe with three columns: WORD, CONSTRUCTION1 and CONSTRUCTION2. The values in the WORD column are the row names of the input.matrix dataframe.
output.table <- data.frame(output.table, PREF=relations, LLR=log.likelihood.values, PRES=pearson.residuals,
LOR=log.odds.ratios, MI=mi.scores,
DPC2W=delta.p.constr.cues.word, DPW2C=delta.p.word.cues.constr, row.names=NULL)
# This line adds additional columns to the output.table dataframe: PREFERENCE, LLR, PEARSONRESID, LOGODDSRATIO, MI, DELTAPC2W, and DELTAPW2C.
assoc <- cbind(log.likelihood.values, pearson.residuals, log.odds.ratios, mi.scores, delta.p.constr.cues.word, delta.p.word.cues.constr)
if (fye.mpfr=="yes") {
output.table <- data.frame(output.table,
# FYE=sapply(FYE.values, formatMpfr, digits=12))
FYE=sapply(sapply(FYE.values, \(af) -log10(af)), asNumeric))
fisher.scores <- sapply(sapply(FYE.values, \(af) -log10(af)), asNumeric)
assoc <- cbind(log.likelihood.values, pearson.residuals, log.odds.ratios, mi.scores, delta.p.constr.cues.word, delta.p.word.cues.constr, fisher.scores)
}
# If fye.mpfr is set to "yes" then this will add FYE column to output.table dataframe.
colnames(output.table)[2:3] <- c(construction1.name, construction2.name)
# This line changes the column names of CONSTRUCTION1 and CONSTRUCTION2 to the values of construction2.name and construction1.name respectively.
output.table <- output.table[order(output.table$PREF, -output.table$LOR),]
# This line orders the output.table dataframe by the values in PREFERENCE column in ascending order and LOGODDSRATIO in descending order.
write.table(output.table, file=file.path(file_output), sep="\t", row.names=FALSE, col.names=TRUE, quote=FALSE)
# This line saves the output.table dataframe to a tab-separated file in file_output path with the row names and column names in the file.
cat("\n\nThe results are in the file called ", basename(file_output), "and can be found in: ", file_output, ".\n")
plot(log2(output.table[,2]+output.table[,3]), output.table$LOR, type="n",
xlab="Logged co-occurrence frequency", ylab="Association (log odds ratio)")
grid(); abline(h=0, lty=2); abline(v=0, lty=2)
text(log2(output.table[,2]+output.table[,3]), output.table$LOR, output.table$WORD, font=1)
assoc.cor <- cor(assoc, method="kendall")
assoc.cor <- round(assoc.cor, 3)
write.table(assoc.cor, file=file.path(project_dir, "results_analysis", str_glue("assoc_cor_{basename(file_output)}.csv")), sep="\t", row.names=TRUE, col.names=NA, quote=FALSE)
# corrgram(assoc, order = TRUE, lower.panel = panel.shade, upper.panel = panel.pie)
# corrgram(assoc, order = TRUE, lower.panel = panel.ellipse, upper.panel = panel.pts)
}
dist.collexemes()
# R Scripts that auto runs a collostructional analysis on a PropBank-annotated corpus and generates the necessary files for analysis
# Import necessary libraries and install if necessary
library(jsonlite)
library(dplyr)
library(here)
library(tidyr)
library(stringr)
library(rlang)
# Get the path to the project root directory
project_dir <- here::here()
# Run the files automatically
source(file.path(project_dir, "scripts", "utils.R")) # loads several functions used during data processing and analysis
source(file.path(project_dir, "scripts", "load_data.R")) # loads the .json and turns it into a workable data frame
source(file.path(project_dir, "scripts", "clean_data.R")) # cleans the data and creates data frames used for analysis
source(file.path(project_dir, "scripts", "coll.analysis.r")) # Loads functions needed for the DCA # Copyright (C) 2022 Stefan Th. Gries (Latest changes in this version: 21 August 2022)
##########################################################################################################################################
# IMPORTANT: The run.analysis.auto script does not use the FYE association score by default.
# If you want to use FYE, you have to manually turn it on in the script itself (= change the indicated variable to "yes" instead of "no")
# WARNING: It takes a very long time to perform FYE.
##########################################################################################################################################
source(file.path(project_dir, "scripts", "run.analysis.auto.R")) # Runs the DCA
source(file.path(project_dir, "scripts", "clean_analysis.R"))
get_utterances_by_roleset(corpus_df, "sell.O1")
get_utterances_by_roleset(corpus_df, "sell.01")
get_utterances_by_roleset(corpus_df, "extend.01")
# R Scripts that auto runs a collostructional analysis on a PropBank-annotated corpus and generates the necessary files for analysis
# Import necessary libraries and install if necessary
library(jsonlite)
library(dplyr)
library(here)
library(tidyr)
library(stringr)
library(rlang)
# Get the path to the project root directory
project_dir <- here::here()
# Run the files automatically
source(file.path(project_dir, "scripts", "utils.R")) # loads several functions used during data processing and analysis
source(file.path(project_dir, "scripts", "load_data.R")) # loads the .json and turns it into a workable data frame
source(file.path(project_dir, "scripts", "clean_data.R")) # cleans the data and creates data frames used for analysis
source(file.path(project_dir, "scripts", "coll.analysis.r")) # Loads functions needed for the DCA # Copyright (C) 2022 Stefan Th. Gries (Latest changes in this version: 21 August 2022)
##########################################################################################################################################
# IMPORTANT: The run.analysis.auto script does not use the FYE association score by default.
# If you want to use FYE, you have to manually turn it on in the script itself (= change the indicated variable to "yes" instead of "no")
# WARNING: It takes a very long time to perform FYE.
##########################################################################################################################################
source(file.path(project_dir, "scripts", "run.analysis.auto.R")) # Runs the DCA
# R Scripts that auto runs a collostructional analysis on a PropBank-annotated corpus and generates the necessary files for analysis
# Import necessary libraries and install if necessary
library(jsonlite)
library(dplyr)
library(here)
library(tidyr)
library(stringr)
# Get the path to the project root directory
project_dir <- here::here()
# Run the files automatically
source(file.path(project_dir, "scripts", "utils.R")) # loads several functions used during data processing and analysis
source(file.path(project_dir, "scripts", "load_data.R")) # loads the .json and turns it into a workable data frame
source(file.path(project_dir, "scripts", "clean_data.R")) # cleans the data and creates data frames used for analysis
source(file.path(project_dir, "scripts", "coll.analysis.r")) # Loads functions needed for the DCA # Copyright (C) 2022 Stefan Th. Gries (Latest changes in this version: 21 August 2022)
##########################################################################################################################################
# IMPORTANT: The run.analysis.auto script does not use the FYE association score by default.
# If you want to use FYE, you have to manually turn it on in the script itself (= change the indicated variable to "yes" instead of "no")
# WARNING: It takes a very long time to perform FYE.
##########################################################################################################################################
source(file.path(project_dir, "scripts", "run.analysis.auto.R")) # Runs the DCA
source(file.path(project_dir, "scripts", "clean_analysis.R"))
# This code imports the necessary libraries, reads the data file, extracts the roles and rolesets, and adds a column to the data frame called argument_structure_construction. The function add_arg_struct takes the roles data frame and extracts the roleType and pos columns from the roles data frame. It then concatenates the roleType and pos columns into a string and collapses the role strings into a single string separated by '-'. It then adds the argument_structure_construction column to the corpus dataframe.
# Import necessary libraries
library(jsonlite)
library(dplyr)
library(here)
library(tidyr)
library(stringr)
# Get the path to the project root directory
project_dir <- here::here()
# Use the project root directory as a base for other file paths
raw_data_file <- file.path(project_dir, "data", "raw_data", "CCxG_Ditrans_10000.json")
# Load the CCxG propbank-annotated .json corpus data
corpus_df <- fromJSON(raw_data_file)
# Convert JSON data to data frame
corpus_df <- as.data.frame(corpus_df)
# Extract the roles from the data frame
roles <- corpus_df[["roles"]]
# Extract the rolesets from the roles data frame
roleset_df <- data.frame(roleset = unlist(lapply(roles, function(x) x$roleset)))
# Remove NA values
roleset_df <- na.omit(roleset_df)
# Extract the rolesets from the roles data frame
lemma_df <- data.frame(lemma = unlist(lapply(roles, function(x) x$lemma)))
# Remove NA values
lemma_df <- na.omit(lemma_df)
add_lemma_column <- function(corpus_df, lemma_df) {
corpus_df$lemma <- lemma_df$lemma
return(corpus_df)
}
corpus_df <- add_lemma_column(corpus_df, lemma_df)
# Function that adds an additional column to a dataframe
# The new column is called argument_structure_construction and is a combination of roleType and pos
add_arg_struct <- function(data_frame) {
# Add a new column called 'argument_structure_construction' to the data frame
data_frame$argument_structure_construction <- sapply(data_frame$roles, function(x) {
# Concatenate the roleType and pos columns into a string like 'arg0(np)'
role_strings <- paste0(x$roleType, "(", x$pos, ")")
# Collapse the role strings into a single string separated by '-'
return(paste(role_strings, collapse = "-"))
})
# Return the modified data frame
return(data_frame)
}
# Run the add add_arg_struct function to the corpus dataframe
corpus_df <- add_arg_struct(corpus_df)
# Function that creates the dataframe used for the analysis
# It combines the argument structure and roleset into a dataframe
combine_arg_struct_and_roleset <- function(corpus, rolesets, lemmas) {
corpus_analysis_df <- data.frame(arg_struc_cxn = corpus$argument_structure_construction, roleset = rolesets$roleset, lemma = lemmas$lemma)
return(corpus_analysis_df)
}
# Combine the corpus data with the rolesets and lemmas data
corpus_analysis_df <- combine_arg_struct_and_roleset(corpus_df, roleset_df, lemma_df)
# Write the combined data to a file
write.table(corpus_analysis_df, file = file.path(project_dir, "data", "raw_data", "raw_corpus_data.csv"), row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
# Use the project root directory as a base for other file paths
corpus_cleaned_fp <- file.path(project_dir, "data", "corpus_cleaned")
# Detect the rolesets used by each lemma
lemma_rolesets_df <- detect_rolesets_by_lemma(corpus_analysis_df)
# Count the frequency of each argument structure, roleset, and lemma
freq_list <- count_frequency(corpus_analysis_df)
# Create a data frame with the number of types for each of the three categories
numbers_df <- data.frame(lemma = integer(), roleset = integer(), arg_struc_cxn = integer(), stringsAsFactors = FALSE)
numbers_df <- numbers_df %>% add_row(lemma = length(freq_list[[3]]), roleset = length(freq_list[[2]]), arg_struc_cxn = length(freq_list[[1]]))
rownames(numbers_df) <- c("#types")
# Save the results in a .csv file
write.table(numbers_df, file = file.path(corpus_cleaned_fp, "numbers.csv"), row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
# Combine the argument structure, roleset, and lemma columns to create a data frame with all possible combinations of these three categories
df_cxn_lemma_multiple <- combine_columns(corpus_analysis_df, corpus_analysis_df, "arg_struc_cxn", "lemma")
write.table(df_cxn_lemma_multiple, file = file.path(corpus_cleaned_fp, "cxn_lemma_multiple.csv"), row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
df_cxn_roleset_multiple <- combine_columns(corpus_analysis_df, corpus_analysis_df, "arg_struc_cxn", "roleset")
write.table(df_cxn_roleset_multiple, file = file.path(corpus_cleaned_fp, "cxn_roleset_multiple.csv"), row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
# Filter the data frame to only include argument structures that occur more than once
df_cxn_roleset_lemma_distinctive <- filter_by_n_most_frequent_arg_struc_cxn(corpus_analysis_df, 2)
# Combine the argument structure, roleset, and lemma columns to create a data frame with all possible combinations of these three categories
df_cxn_lemma_distinctive <- combine_columns(df_cxn_roleset_lemma_distinctive, df_cxn_roleset_lemma_distinctive, "arg_struc_cxn", "lemma")
write.table(df_cxn_lemma_distinctive, file = file.path(corpus_cleaned_fp, "cxn_lemma_distinctive.csv"), row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
df_cxn_roleset_distinctive <- combine_columns(df_cxn_roleset_lemma_distinctive, df_cxn_roleset_lemma_distinctive, "arg_struc_cxn", "roleset")
write.table(df_cxn_roleset_distinctive, file = file.path(corpus_cleaned_fp, "cxn_roleset_distinctive.csv"), row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
#
# Get a list of the unique construction types in the corpus
unique_cxn_list <- unique(corpus_analysis_df$arg_struc_cxn)
# Create a new dataframe with the lemma and roleset for each unique lemma
df_summary_corpus <- corpus_analysis_df %>%
group_by(lemma) %>%
summarise(roleset)
# Count the frequency of each roleset in each construction
construction_counts <- corpus_analysis_df %>%
group_by(lemma, roleset, arg_struc_cxn) %>%
summarize(count = n())
# Pivot the dataframe
df_summary_corpus <- pivot_wider(construction_counts, names_from = arg_struc_cxn, values_from = count)
# Sum the total number of constructions for each lemma
df_summary_corpus$total <- rowSums(df_summary_corpus[,3:ncol(df_summary_corpus)], na.rm = TRUE)
# Create a dataframe of the construction types in descending order of frequency
cxn_freq_table <- data.frame(sort(freq_list[[1]], decreasing = TRUE, by = "value"))
colnames(cxn_freq_table)[1]<-"arg_struc_cxn"
cxn_freq_df <- data.frame(cxn_freq_table$arg_struc_cxn)
col_order <- c(cxn_freq_df$cxn_freq_table.arg_struc_cxn)
# Create a new dataframe with the lemmas and constructions in descending order of frequency
alph_lemma_df <- select(df_summary_corpus, "lemma", "roleset", "total", c(col_order))
# Write the dataframe to a file
write.table(alph_lemma_df, file = file.path(corpus_cleaned_fp, "alph_lemma_roleset_cxn.csv"), row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
# Create another dataframe with the lemmas and constructions in descending order of frequency
df_summary_corpus <- df_summary_corpus %>%
arrange(desc(total))
# Create a new dataframe of the construction types in descending order of frequency
corpus_cleaned_overview <- select(df_summary_corpus, "lemma", "roleset", "total", c(col_order))
write.table(corpus_cleaned_overview, file = file.path(corpus_cleaned_fp, "corpus_cleaned_summary.csv"), row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
write.table(cxn_freq_table, file = file.path(corpus_cleaned_fp, "cxn_frequency_table.csv"), row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
dist.collexemes <- function(precbitsexponent = precbitsexponent) {
# Suppress warning messages
options(warn = -1)
# Initialize variables
dists <- 1
input.dc <- 1
fye.mpfr <- "no" # change input to "yes" if you want the FYE calculated as well. Warning: this takes a long time!
# Get the base name of the file (without the file extension)
file_input_base <- basename(file_input)
# Create the filepath for the output file
file_output <- file.path(project_dir, "results_analysis", "reproduced_analyses", file_input_base)
file_output <- paste0(gsub("\\..*", "", file_output), "_reproduced_analysis.csv")
# Read the input file as a table
input.matrix <- read.table(file_input, header = TRUE, sep = "\t", quote = "", comment.char = "")
if (input.dc == 1) { # DCA
# Transpose the table
interim <- t(table(input.matrix))
# Convert the table to a dataframe
input.matrix <- data.frame(
as.vector(interim[,1]),
as.vector(interim[,2]), row.names = rownames(interim))
# Rename the columns
colnames(input.matrix) <- colnames(interim)
} else {
temp <- colnames(input.matrix)[2:3]
# Convert the table to a dataframe
input.matrix <- data.frame(
as.vector(input.matrix[,2]),
as.vector(input.matrix[,3]), row.names = input.matrix[,1])
# Rename the columns
colnames(input.matrix) <- temp
}
# Get the name of the first construction
construction1.name <- colnames(input.matrix)[1]; construction2.name <- colnames(input.matrix)[2]
# computation
options(warn=-1)
# calculate Pearson's residuals for each word-construction pair
pearson.residuals <- chisq.test(input.matrix, correct=FALSE)$residuals[,1]
# create a list of 2x2 matrices for each word-construction pair
all.2.by.2.matrices <- apply(
input.matrix, 1,
\(af) { matrix(c(af, colSums(input.matrix)-af), byrow=TRUE, ncol=2) },
simplify=FALSE)
# if FYE calculation is desired, calculate FYE values for each 2x2 matrix
if (fye.mpfr=="yes") {
FYE.values <- lapply(all.2.by.2.matrices,
\(af) fisher.test.mpfr(af))
}
# calculate log-odds ratios for each 2x2 matrix
glms <- lapply(all.2.by.2.matrices,
\(af) glm(rbind(af[1,], af[2,]) ~ c(1:2), family=binomial))
log.odds.ratios <- -sapply(glms, coefficients)[2,]
# calculate log-likelihood values for each 2x2 matrix
log.likelihood.values <- sapply(glms, "[[", "null.deviance")
# calculate MI scores for each 2x2 matrix
mi.scores <- sapply(all.2.by.2.matrices,
\(af) log2(af[1,1] / chisq.test(af, correct=FALSE)$exp[1,1]))
# calculate delta p values for each 2x2 matrix
delta.p.constr.cues.word <- sapply(all.2.by.2.matrices,
\(af) af[1,1]/sum(af[,1]) - af[1,2]/sum(af[,2]))
delta.p.word.cues.constr <- sapply(all.2.by.2.matrices,
\(af) af[1,1]/sum(af[1,]) - af[2,1]/sum(af[2,]))
# determine whether each word-construction pair is associated or not
relations <- sapply(pearson.residuals,
\(af) switch(sign(af)+2, construction2.name, "chance", construction1.name))
# output
output.table <- data.frame(WORD=rownames(input.matrix), CONSTRUCTION1=input.matrix[,1], CONSTRUCTION2=input.matrix[,2], row.names=NULL)
# This line creates a dataframe with three columns: WORD, CONSTRUCTION1 and CONSTRUCTION2. The values in the WORD column are the row names of the input.matrix dataframe.
output.table <- data.frame(output.table, PREF=relations, LLR=log.likelihood.values, PRES=pearson.residuals,
LOR=log.odds.ratios, MI=mi.scores,
DPC2W=delta.p.constr.cues.word, DPW2C=delta.p.word.cues.constr, row.names=NULL)
# This line adds additional columns to the output.table dataframe: PREFERENCE, LLR, PEARSONRESID, LOGODDSRATIO, MI, DELTAPC2W, and DELTAPW2C.
assoc <- cbind(log.likelihood.values, pearson.residuals, log.odds.ratios, mi.scores, delta.p.constr.cues.word, delta.p.word.cues.constr)
if (fye.mpfr=="yes") {
output.table <- data.frame(output.table,
# FYE=sapply(FYE.values, formatMpfr, digits=12))
FYE=sapply(sapply(FYE.values, \(af) -log10(af)), asNumeric))
fisher.scores <- sapply(sapply(FYE.values, \(af) -log10(af)), asNumeric)
assoc <- cbind(log.likelihood.values, pearson.residuals, log.odds.ratios, mi.scores, delta.p.constr.cues.word, delta.p.word.cues.constr, fisher.scores)
}
# If fye.mpfr is set to "yes" then this will add FYE column to output.table dataframe.
colnames(output.table)[2:3] <- c(construction1.name, construction2.name)
# This line changes the column names of CONSTRUCTION1 and CONSTRUCTION2 to the values of construction2.name and construction1.name respectively.
output.table <- output.table[order(output.table$PREF, -output.table$LOR),]
# This line orders the output.table dataframe by the values in PREFERENCE column in ascending order and LOGODDSRATIO in descending order.
write.table(output.table, file=file.path(file_output), sep="\t", row.names=FALSE, col.names=TRUE, quote=FALSE)
# This line saves the output.table dataframe to a tab-separated file in file_output path with the row names and column names in the file.
cat("\n\nThe results are in the file called ", basename(file_output), "and can be found in: ", file_output, ".\n")
plot(log2(output.table[,2]+output.table[,3]), output.table$LOR, type="n",
xlab="Logged co-occurrence frequency", ylab="Association (log odds ratio)")
grid(); abline(h=0, lty=2); abline(v=0, lty=2)
text(log2(output.table[,2]+output.table[,3]), output.table$LOR, output.table$WORD, font=1)
assoc.cor <- cor(assoc, method="kendall")
assoc.cor <- round(assoc.cor, 3)
write.table(assoc.cor, file=file.path(project_dir, "results_analysis", "reproduced_analyses", str_glue("assoc_cor_{basename(file_output)}.csv")), sep="\t", row.names=TRUE, col.names=NA, quote=FALSE)
# corrgram(assoc, order = TRUE, lower.panel = panel.shade, upper.panel = panel.pie)
# corrgram(assoc, order = TRUE, lower.panel = panel.ellipse, upper.panel = panel.pts)
}
file_input = file.path(project_dir, "data", "corpus_cleaned", "cxn_lemma_distinctive.csv")
dist.collexemes()
file_input = file.path(project_dir, "data", "corpus_cleaned", "cxn_roleset_distinctive.csv")
dist.collexemes()
# Use the project root directory as a base for other file paths
analysis_fp <- file.path(project_dir, "results_analysis", "reproduced_analyses")
# Load the roleset csv file
results_roleset_cxnDT <- read.csv(file.path(analysis_fp, "cxn_roleset_distinctive_reproduced_analysis.csv"), header=TRUE, sep="\t", quote="", comment.char="")
# Create an empty list to store the new dataframes
cxn_roleset_preference_list = list()
# Get the unique constructions in the "PREF" column
constructions = unique(results_roleset_cxnDT$PREF)
# Create data frame with only constructions that are relevant to the analysis
# This is necessary because the results_roleset_cxnDT data frame is not filtered by construction
constructions_df = data.frame(constructions)
# Assign the first construction in the constructions vector to cxn1
cxn1 = constructions_df$constructions[1]
# Assign the second construction in the constructions vector to cxn2
cxn2 = constructions_df$constructions[2]
# Rename the third column in the results_roleset_cxnDT data frame to the first construction
colnames(results_roleset_cxnDT)[2] <- cxn1
# Rename the fourth column in the results_roleset_cxnDT data frame to the second construction
colnames(results_roleset_cxnDT)[3] <- cxn2
# Filter the results_roleset_cxnDT data frame to show only the rows with the constructions in the constructions vector
mix_results_all_cxn_roleset <- results_roleset_cxnDT %>%
filter(!!as.symbol(cxn1) > 0 & !!as.symbol(cxn2) > 0)
# Order by LLR
mix_results_all_cxn_roleset <- mix_results_all_cxn_roleset[order(-mix_results_all_cxn_roleset$LLR), ]
write.table(mix_results_all_cxn_roleset, file = file.path(analysis_fp, str_glue("results_cxn12_mixed_roleset_reproduced_analysis.csv")), row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
# Iterate through each construction
for (c in constructions) {
# Subset the original dataframe to only include rows with the current construction
subset_df = results_roleset_cxnDT[results_roleset_cxnDT$PREF == c, ]
# Sort the data frame based on the LOGODDSRATIO column
subset_df = subset_df[order(-subset_df$LLR), ]
# Assign the subsetted dataframe to a new object named after the construction
assign(c, subset_df)
# Append the subsetted dataframe to the list of new dataframes
cxn_roleset_preference_list[[c]] <- subset_df
}
write.table(cxn_roleset_preference_list[[cxn1]], file = file.path(analysis_fp, str_glue("results_{cxn1}_roleset_distinctive_reproduced_analysis.csv")), row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
write.table(cxn_roleset_preference_list[[cxn2]], file = file.path(analysis_fp, str_glue("results_{cxn2}_roleset_distinctive_reproduced_analysis.csv")), row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
# Load the lemma csv file
results_lemma_cxnDT <- read.csv(file.path(analysis_fp, "cxn_lemma_distinctive_reproduced_analysis.csv"), header=TRUE, sep="\t", quote="", comment.char="")
colnames(results_lemma_cxnDT)[2] <- cxn1
colnames(results_lemma_cxnDT)[3] <- cxn2
mix_results_all_cxn_lemma <- results_lemma_cxnDT %>%
filter(!!as.symbol(cxn1) > 0 & !!as.symbol(cxn2) > 0)
mix_results_all_cxn_lemma <- mix_results_all_cxn_lemma[order(-mix_results_all_cxn_lemma$LLR), ]
write.table(mix_results_all_cxn_lemma, file = file.path(analysis_fp, str_glue("results_cxn12_mixed_lemma_reproduced_analysis.csv")), row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
# Create an empty list to store the new dataframes
cxn_lemma_preference_list = list()
# Iterate through each construction
for (c in constructions) {
# Subset the original dataframe to only include rows with the current construction
subset_df = results_lemma_cxnDT[results_lemma_cxnDT$PREF == c, ]
# Sort the data frame based on the LOGODDSRATIO column
subset_df = subset_df[order(-subset_df$LLR), ]
# Assign the subsetted dataframe to a new object named after the construction
assign(c, subset_df)
# Append the subsetted dataframe to the list of new dataframes
cxn_lemma_preference_list[[c]] <- subset_df
}
write.table(cxn_lemma_preference_list[[cxn1]], file = file.path(analysis_fp, str_glue("results_{cxn1}_lemma_distinctive_reproduced_analysis.csv")), row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
write.table(cxn_lemma_preference_list[[cxn2]], file = file.path(analysis_fp, str_glue("results_{cxn2}_lemma_distinctive_reproduced_analysis.csv")), row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
######
# export the same lists but only with verbs that occur in both constructions
# Load the roleset csv file
mix_results_roleset <- read.table(file.path(analysis_fp, str_glue("results_{cxn1}_roleset_distinctive_reproduced_analysis.csv")), header=TRUE, sep="\t", quote="", comment.char="")
mix_results_roleset <- data.frame(mix_results_roleset)
colnames(mix_results_roleset)[2] <- cxn1
colnames(mix_results_roleset)[3] <- cxn2
cxn1_mixed <- colnames(mix_results_roleset)[2]
cxn2_mixed <- colnames(mix_results_roleset)[3]
# Filter to only larger than 0 in cxn column
mix_results_roleset <- mix_results_roleset %>%
filter(!!as.symbol(cxn1_mixed) > 0 & !!as.symbol(cxn2_mixed) > 0)
write.table(mix_results_roleset, file = file.path(analysis_fp, str_glue("results_{cxn1}_mixed_rolesets_reproduced_analysis.csv")), row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
# Load the roleset csv file
mix_results_roleset <- read.table(file.path(analysis_fp, str_glue("results_{cxn2}_roleset_distinctive_reproduced_analysis.csv")), header=TRUE, sep="\t", quote="", comment.char="")
mix_results_roleset <- data.frame(mix_results_roleset)
colnames(mix_results_roleset)[2] <- cxn1
colnames(mix_results_roleset)[3] <- cxn2
# Filter to only larger than 0 in cxn column
mix_results_roleset <- mix_results_roleset %>%
filter(!!as.symbol(cxn1_mixed) > 0 & !!as.symbol(cxn2_mixed) > 0)
write.table(mix_results_roleset, file = file.path(analysis_fp, str_glue("results_{cxn2}_mixed_rolesets_reproduced_analysis.csv")), row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
# Load the lemma csv file
mix_results_lemma <- read.table(file.path(analysis_fp, str_glue("results_{cxn1}_lemma_distinctive_reproduced_analysis.csv")), header=TRUE, sep="\t", quote="", comment.char="")
mix_results_lemma <- data.frame(mix_results_lemma)
colnames(mix_results_lemma)[2] <- cxn1
colnames(mix_results_lemma)[3] <- cxn2
cxn1_mixed <- colnames(mix_results_roleset)[2]
cxn2_mixed <- colnames(mix_results_roleset)[3]
# Filter to only larger than 0 in cxn column
mix_results_lemma <- mix_results_lemma %>%
filter(!!as.symbol(cxn1_mixed) > 0 & !!as.symbol(cxn2_mixed) > 0)
write.table(mix_results_lemma, file = file.path(analysis_fp, str_glue("results_{cxn1}_mixed_lemma_reproduced_analysis.csv")), row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
# Load the lemma csv file
mix_results_lemma <- read.table(file.path(analysis_fp, str_glue("results_{cxn2}_lemma_distinctive_reproduced_analysis.csv")), header=TRUE, sep="\t", quote="", comment.char="")
mix_results_lemma <- data.frame(mix_results_lemma)
colnames(mix_results_lemma)[2] <- cxn1
colnames(mix_results_lemma)[3] <- cxn2
# Filter to only larger than 0 in cxn column
mix_results_lemma <- mix_results_lemma %>%
filter(!!as.symbol(cxn1_mixed) > 0 & !!as.symbol(cxn2_mixed) > 0)
write.table(mix_results_lemma, file = file.path(analysis_fp, str_glue("results_{cxn2}_mixed_lemma_reproduced_analysis.csv")), row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
corpus_df$argument_structure_construction <- sapply(corpus_df$roles, function(x) paste0(x$roleType, "(", x$pos, ")"), collapse = "-")
# R Scripts that auto runs a collostructional analysis on a PropBank-annotated corpus and generates the necessary files for analysis
# Import necessary libraries and install if necessary
library(jsonlite)
library(dplyr)
library(here)
library(tidyr)
library(stringr)
# Get the path to the project root directory
project_dir <- here::here()
# Run the files automatically
source(file.path(project_dir, "scripts", "utils.R")) # loads several functions used during data processing and analysis
source(file.path(project_dir, "scripts", "load_data.R")) # loads the .json and turns it into a workable data frame
source(file.path(project_dir, "scripts", "clean_data.R")) # cleans the data and creates data frames used for analysis
source(file.path(project_dir, "scripts", "coll.analysis.r")) # Loads functions needed for the DCA # Copyright (C) 2022 Stefan Th. Gries (Latest changes in this version: 21 August 2022)
##########################################################################################################################################
# IMPORTANT: The run.analysis.auto script does not use the FYE association score by default.
# If you want to use FYE, you have to manually turn it on in the script itself (= change the indicated variable to "yes" instead of "no")
# WARNING: It takes a very long time to perform FYE.
##########################################################################################################################################
source(file.path(project_dir, "scripts", "run.analysis.auto.R")) # Runs the DCA
source(file.path(project_dir, "scripts", "clean_analysis.R"))
