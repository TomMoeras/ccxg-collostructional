xlab="Logged co-occurrence frequency", ylab="Association (log odds ratio)")
grid(); abline(h=0, lty=2); abline(v=0, lty=2)
text(log2(output.table[,2]+output.table[,3]), output.table$LOGODDSRATIO, output.table$WORD, font=3)
} else { # MDCA
# introduction
cat("\nFor a multiple distinctive collexeme analysis, this version of coll.analysis expects as input\na file with a table of all tokens and returns collexeme strengths based on Pearson residuals. That is, the first column contains for\neach co-occurrence item the code for one of the X words/constructions W/C\nyou want to investigate; the second column contains the word co-occurring with W/C\nas listed in the first column.\n\nW/C\tColl_Word\nA\tX\nB\tY\nC\tZ\n...\t...\n\nYour file ideally has no spaces (use '_' instead) and don't forget that R's treatment of alphanumeric characters\nis case-sensitive!\n\nChoose the text file with the input data!\t"); pause()
# input
cat("\nLoad the tab-delimited input file:\n"); pause()
input.matrix <- read.table(file.choose(), header=TRUE, sep="\t", quote="", comment.char="")
names(input.matrix) <- c("W_C", "Coll_Word")
input.matrix <- table(input.matrix$Coll_Word, input.matrix$W_C)
# computation
pearson.residuals <- as.data.frame.matrix(chisq.test(input.matrix, correct=FALSE)$residuals)
output.table <- data.frame(COLLOCATE=rownames(pearson.residuals), pearson.residuals,
SUMABSDEV=apply(pearson.residuals, 1, \(af) sum(abs(af))),
LARGESTPREF=colnames(pearson.residuals)[apply(pearson.residuals, 1, \(af) which.max(af))])
output.table <- output.table[order(output.table$LARGESTPREF, -output.table$SUMABSDEV),]
write.table(output.table, file=save.date <- gsub(":", "-", paste0(Sys.time(), ".csv")), sep="\t", row.names=FALSE, col.names=TRUE, quote=FALSE)
cat("\n\nThe results are in the file called ", save.date, ".\n")
}
options(warn=0)
} # END OF FUNCTION FOR DISTINCTIVE COLLEXEME ANALYSIS
covar.collexemes<-function(precbitsexponent=precbitsexponent) { # FUNCTION FOR CO-VARYING COLLEXEME ANALYSIS
options(warn=-1)
# introduction
cat("\nC o v a r y i n g   c o l l e x e m e   a n a l y s i s   . . .\n\n\nThis kind of analysis investigated dependencies within two slots of a single construction.\nThis script so far only implements the so-called item-based analysis since comparative studies\n have shown that the system-based correction may require many days computational time with only minor differences in the results (cf. Stefanowitsch and Gries 2005). However, somewhere down the road I may find \ntime to work on an implementation of this technique so that arbitrarily many additional variables\n(e.g. register, corpora etc.) can be included.\n\nColl.analysis 3.2a requires as input for the item-based co-varying collexeme analysis:\na file with a table of all token instances of the construction C with\nthe two words W1 and W2 occurring in the slots of each instance of C.That is, you need the following kind of input file (with column names!)),\nwhere the number of rows corresponds to the number of construction tokens you have.\n\nWord_Slot1\tWord_Slot2\nA\t\tX\nB\t\tX\n...\t...\n\nYour file must not have decimal points/separators and ideally has no spaces (for the latter, use '_' instead)!\nAlso, don't forget that R's treatment of alphanumeric characters is case-sensitive!\n\n")
# input
cat("\nLoad the tab-delimited input file:\n"); pause()
input.data <- read.table(file.choose(), header=TRUE, sep="\t", quote="", comment.char="")
output.table <- as.data.frame(table(input.data))
output.table$FREQOFSLOT1 <- table(input.data$WORD_SLOT1)[output.table[,1]]
output.table$FREQOFSLOT2 <- table(input.data$WORD_SLOT2)[output.table[,2]]
construction.freq <- sum(output.table[,3])
# computation
all.2.by.2.matrices <- apply(
output.table[,3:5], 1,
\(af) { matrix(c(af[1], af[2]-af[1], af[3]-af[1], construction.freq-af[1]-(af[2]-af[1])-(af[3]-af[1])), byrow=TRUE, ncol=2) },
simplify=FALSE)
glms <- lapply(all.2.by.2.matrices,
\(af) glm(rbind(af[1,], af[2,]) ~ c(1:2), family=binomial))
log.odds.ratios <- -sapply(glms, coefficients)[2,]
log.likelihood.values <- sapply(glms, "[[", "null.deviance") * sign(log.odds.ratios)
mi.scores <- sapply(all.2.by.2.matrices,
\(af) log2(af[1,1] / chisq.test(af, correct=FALSE)$exp[1,1]))
delta.p.slot1.cues.slot2 <- sapply(all.2.by.2.matrices,
\(af) af[1,1]/sum(af[1,]) - af[2,1]/sum(af[2,]))
delta.p.slot2.cues.slot1 <- sapply(all.2.by.2.matrices,
\(af) af[1,1]/sum(af[,1]) - af[1,2]/sum(af[,2]))
relations <- sapply(log.odds.ratios,
\(af) switch(sign(af)+2, "repulsion", "chance", "attraction"))
# output
output.table <- data.frame(output.table, RELATION=relations, LLR=log.likelihood.values,
LOGODDSRATIO=log.odds.ratios, MI=mi.scores,
DELTAP1TO2=delta.p.slot1.cues.slot2, DELTAP2TO1=delta.p.slot2.cues.slot1, row.names=NULL)
output.table <- output.table[order(output.table$RELATION, -output.table$LOGODDSRATIO),]
write.table(output.table, file=save.date <- gsub(":", "-", paste0(Sys.time(), ".csv")), sep="\t", row.names=FALSE, col.names=TRUE, quote=FALSE)
cat("\n\nThe results are in the file called ", save.date, ".\n")
options(warn=0)
} # END OF FUNCTION FOR CO-VARYING COLLEXEME ANALYSIS
pause <- function() { cat("Press <Enter> to continue ... "); readline(); invisible() }
fisher.test.mpfr <- function (some.2by2.matrix, precBits=1000) {
options(warn=-1)
stopifnot("Rmpfr" %in% rownames(installed.packages()))
library(Rmpfr, warn.conflicts=FALSE, quietly=TRUE, verbose=FALSE)
# define helper functions
binomcoeff.mpfr <- function (k.successes, n.trials, precBits=1000) {
numerator <-   factorial(mpfr(n.trials,             precBits=precBits))
denominator <- factorial(mpfr(k.successes,          precBits=precBits)) *
factorial(mpfr(n.trials-k.successes, precBits=precBits))
return(numerator/denominator)
}
hypergeom.mpfr <- function (a, a.and.b, a.and.c, n, precBits=1000) {
numerator.1 <- binomcoeff.mpfr(a        , a.and.b        , precBits=precBits)
numerator.2 <- binomcoeff.mpfr(a.and.c-a, n      -a.and.b, precBits=precBits)
denominator <- binomcoeff.mpfr(a.and.c  , n              , precBits=precBits)
return((numerator.1*numerator.2)/denominator)
}
range.of.as <- 0:min(rowSums(some.2by2.matrix)[1], colSums(some.2by2.matrix)[1])
if ("doParallel" %in% rownames(installed.packages())) { # if doParallel is installed
library(doParallel); registerDoParallel(cl <- makePSOCKcluster(detectCores()-1, outfile=""))
all.results <- # make all.results the result of doing w/ a cluster
foreach(i=seq(range.of.as), .packages=c("Rmpfr"), .verbose=FALSE) %dopar% { cat(".")
curr.matrix <- matrix(c(
a <- range.of.as[i],                              # a
b <- rowSums(some.2by2.matrix)[1]-range.of.as[i], # b
c <- colSums(some.2by2.matrix)[1]-range.of.as[i], # c
sum(some.2by2.matrix)-sum(a,b,c)),                # d
byrow=TRUE, ncol=2)
# check if the 'normal' way works, because it's so much faster
hypergeom.quick <- dhyper(curr.matrix[1,1], rowSums(curr.matrix)[1], rowSums(curr.matrix)[2], colSums(curr.matrix)[1])
if (hypergeom.quick==0) {
output <- c(
"chi.sq.values"=chisq.test(curr.matrix, correct=FALSE)$statistic,
"pointwise.hypgeom.p"=hypergeom.mpfr(
curr.matrix[1,1],        # a
rowSums(curr.matrix)[1], # a+b
colSums(curr.matrix)[1], # a+c
sum(curr.matrix),        # n
precBits=precBits))      # precision
return(output)
} else {
output <- c(
"chi.sq.values"=chisq.test(curr.matrix, correct=FALSE)$statistic,
"pointwise.hypgeom.p"=mpfr(hypergeom.quick, precBits=precBits))
return(output)
}
}; stopCluster(cl)
chi.sq.values <- sapply(all.results, "[[", 1)
pointwise.hypgeom.p <- sapply(all.results, "[[", 2)
names(pointwise.hypgeom.p) <- names(chi.sq.values) <- range.of.as
} else { # if doParallel is not installed
# set up collectors for the relevant scenario
pointwise.hypgeom.p <- vector(mode="list", length=length(range.of.as))
chi.sq.values <- rep(NA, length(range.of.as))
names(pointwise.hypgeom.p) <- names(chi.sq.values) <- range.of.as
for (i in seq(range.of.as)) {
curr.matrix <- matrix(c(
a <- range.of.as[i],                              # a
b <- rowSums(some.2by2.matrix)[1]-range.of.as[i], # b
c <- colSums(some.2by2.matrix)[1]-range.of.as[i], # c
sum(some.2by2.matrix)-sum(a,b,c)),                # d
byrow=TRUE, ncol=2)
chi.sq.values[i] <- chisq.test(curr.matrix, correct=FALSE)$statistic
# check if the 'normal' way works, because it's so much faster
hypergeom.quick <- dhyper(curr.matrix[1,1], rowSums(curr.matrix)[1], rowSums(curr.matrix)[2], colSums(curr.matrix)[1])
if (hypergeom.quick==0) {
pointwise.hypgeom.p[i] <- hypergeom.mpfr(
curr.matrix[1,1],        # a
rowSums(curr.matrix)[1], # a+b
colSums(curr.matrix)[1], # a+c
sum(curr.matrix),        # n
precBits=precBits)       # precision
} else {
pointwise.hypgeom.p[i] <- mpfr(hypergeom.quick, precBits=precBits)
}
}
}
# compute result
pickito <- which(chi.sq.values >= chi.sq.values[which(names(pointwise.hypgeom.p) == some.2by2.matrix[1,1])])
return(sum(mpfr(pointwise.hypgeom.p)[pickito]))
options(warn=0)
}
coll.analysis()
write.csv(corpus_analysis_df, "corpus_analysis.csv", row.names = FALSE, column.names = FALSE, sep = "_")
write.csv(corpus_analysis_df, "corpus_analysis.csv", row.names = FALSE, sep = "_")
coll.analysis()
coll.analysis()
write.csv(corpus_analysis_df, "corpus_analysis.csv", row.names = FALSE, sep = "\t")
write.table(corpus_analysis_df, "corpus_analysis.csv", row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
coll.analysis()
coll.analysis()
write.table(corpus_analysis_df, "corpus_analysis.csv", row.names = FALSE, sep = "\t", col.names = FALSE, quote = "")
write.table(corpus_analysis_df, "corpus_analysis.csv", row.names = FALSE, sep = "\t", col.names = FALSE)
coll.analysis()
library(mclm)
freqlist(corpus_analysis_df)
freqlist(corpus_df)
View(corpus_df)
View(corpus_df[[2]][[523]])
View(corpus_df[[2]][[87]])
View(corpus_df[[2]][[87]])
View(corpus_analysis_df)
write.table(corpus_analysis_df, "corpus_analysis.csv", row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
coll.analysis()
coll.analysis()
count_frequency <- function(data_frame) {
arg_struc_cxn_table <- table(data_frame$arg_struc_cxn)
roleset_table <- table(data_frame$roleset)
return(list(arg_struc_cxn_table, roleset_table))
}
freq_table <- count_frequency(corpus_analysis_df)
View(freq_table)
filter_by_n_most_frequent_arg_struc_cxn <- function(data_frame, n) {
# Get the counts of the arg_struc_cxn
arg_struc_cxn_counts <- table(data_frame$arg_struc_cxn)
# Sort the counts in descending order
arg_struc_cxn_counts <- sort(arg_struc_cxn_counts, decreasing = TRUE)
# Get the n most frequent arg_struc_cxn
top_n_arg_struc_cxn <- names(arg_struc_cxn_counts)[1:n]
# Filter the dataframe to only include the rows with the top n arg_struc_cxn
filtered_data_frame <- data_frame[data_frame$arg_struc_cxn %in% top_n_arg_struc_cxn,]
return(filtered_data_frame)
}
filtered_freq_table <- filter_by_n_most_frequent_arg_struc_cxn(corpus_analysis_df, 2)
View(filtered_freq_table)
write.table(corpus_analysis_df, "corpus_analysis.csv", row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
write.table(filtered_freq_table, "corpus_analysis.csv", row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
write.table(filtered_freq_table, "corpus_analysis.csv", row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
coll.analysis()
# Import necessary libraries
library(jsonlite)
library(dplyr)
# Load the CCxG propbank-annotated .json corpus data
corpus_df <- fromJSON("C:/Users/tommo/Documents/VS_Code/R_Ditransative/CCxG_Ditrans_10000.json")
# Convert JSON data to data frame
corpus_df <- as.data.frame(corpus_df)
# Extract the roles from the data frame
roles <- corpus_df[["roles"]]
# Extract the rolesets from the roles data frame
roleset_df <- data.frame(roleset = unlist(lapply(roles, function(x) x$roleset)))
# Remove NA values
roleset_df <- na.omit(roleset_df)
# Function that adds an additional column to a dataframe
# The new column is called argument_structure_construction and is a combination of roleType and pos
add_arg_struct <- function(data_frame) {
# Add a new column called 'argument_structure_construction' to the data frame
data_frame$argument_structure_construction <- sapply(data_frame$roles, function(x) {
# Concatenate the roleType and pos columns into a string like 'arg0(np)'
role_strings <- paste0(x$roleType, "(", x$pos, ")")
# Collapse the role strings into a single string separated by '-'
return(paste(role_strings, collapse = "-"))
})
# Return the modified data frame
return(data_frame)
}
# Run the add add_arg_struct function to the corpus dataframe
corpus_df <- add_arg_struct(corpus_df)
# Function that creates the dataframe used for the analysis
# It combines the argument structure and roleset into a dataframe
combine_arg_struct_and_roleset <- function(corpus, rolesets) {
corpus_analysis_df <- data.frame(arg_struc_cxn = corpus$argument_structure_construction, roleset = rolesets$roleset)
return(corpus_analysis_df)
}
corpus_analysis_df <- combine_arg_struct_and_roleset(corpus_df, roleset_df)
freq_table <- count_frequency(corpus_analysis_df)
View(freq_table)
filter_by_n_most_frequent_arg_struc_cxn(corpus_analysis_df, 2)
coll.analysis()
filtered_freq_table <- filter_by_n_most_frequent_arg_struc_cxn(corpus_analysis_df, 2)
write.table(filtered_freq_table, "corpus_analysis.csv", row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
coll.analysis()
View(freq_table)
View(filtered_freq_table)
View(filtered_freq_table)
View(filtered_freq_table)
View(freq_table)
# Function to extract utterances from data frame that have a specific argument structure construction
get_utterances_by_arg_struct <- function(data_frame, arg_struct) {
# Use sapply to check if any of the argument structure constructions in the data frame match the input arg_struct
# and select the corresponding utterance
utterances <- data_frame$utterance[data_frame$arg_struct == arg_struct]
# remove NAs from the resulting utterances dataframe
utterances <- utterances[!is.na(utterances)]
return(utterances)
}
get_utterances_by_arg_struct(corpus_df, arg0(np)-arg1(np)-v(v)-arg2(pp))
get_utterances_by_arg_struct(corpus_df, "arg0(np)-arg1(np)-v(v)-arg2(pp)")
View(corpus_analysis_df)
View(corpus_df)
# Function to extract utterances from data frame that have a specific argument structure construction
get_utterances_by_arg_struct <- function(data_frame, arg_struct) {
# Use sapply to check if any of the argument structure constructions in the data frame match the input arg_struct
# and select the corresponding utterance
utterances <- data_frame$utterance[data_frame$argument_structure_construction == arg_struct]
# remove NAs from the resulting utterances dataframe
utterances <- utterances[!is.na(utterances)]
return(utterances)
}
get_utterances_by_arg_struct(corpus_df, "arg0(np)-arg1(np)-v(v)-arg2(pp)")
# Function to extract utterances from data frame that have a specific argument structure construction
get_utterances_by_arg_struc_cxn <- function(data_frame, arg_struc_cxn) {
# Use sapply to check if any of the rows in the arg_struct column of the data frame have the input arg_struc_cxn
# and select the corresponding utterance, roles and arg_struct
selected_rows <- data_frame[sapply(data_frame$arg_struct, function(x) any(x == arg_struc_cxn)), c("utterance", "roles", "arg_struct")]
return(selected_rows)
}
specific_cxn <- get_utterances_by_arg_struc_cxn(corpus_df, "arg0(np)-arg1(np)-v(v)-arg2(pp)")
# Function to extract utterances from data frame that have a specific argument structure construction
get_utterances_by_arg_struc_cxn <- function(data_frame, arg_struc_cxn) {
# Use sapply to check if any of the rows in the arg_struct column of the data frame have the input arg_struc_cxn
# and select the corresponding utterance, roles and arg_struct
selected_rows <- data_frame[sapply(data_frame$argument_structure_construction, function(x) any(x == arg_struc_cxn)), c("utterance", "roles", "arg_struct")]
return(selected_rows)
}
specific_cxn <- get_utterances_by_arg_struc_cxn(corpus_df, "arg0(np)-arg1(np)-v(v)-arg2(pp)")
View(corpus_df)
# Function to extract utterances from data frame that have a specific argument structure construction
get_utterances_by_arg_struc_cxn <- function(data_frame, arg_struc_cxn) {
# Use sapply to check if any of the rows in the arg_struct column of the data frame have the input arg_struc_cxn
# and select the corresponding utterance, roles and arg_struct
selected_rows <- data_frame[sapply(data_frame$argument_structure_construction, function(x) any(x == arg_struc_cxn)), c("utterance", "roles", "argument_structure_construction")]
return(selected_rows)
}
specific_cxn <- get_utterances_by_arg_struc_cxn(corpus_df, "arg0(np)-arg1(np)-v(v)-arg2(pp)")
View(specific_cxn)
View(specific_cxn[[2]][[2527]])
View(specific_cxn[[2]][[2886]])
print(specific_cxn[[2]][[2886]])
print(specific_cxn[[2]][[1]])
# Function to extract utterances from data frame that have a specific argument structure construction
get_utterances_by_arg_struc_cxn <- function(data_frame, arg_struc_cxn) {
# Use sapply to check if any of the rows in the arg_struct column of the data frame have the input arg_struc_cxn
# and select the corresponding utterance, roles and arg_struct
selected_rows <- data_frame[sapply(data_frame$argument_structure_construction, function(x) any(x == arg_struc_cxn)), c("utterance", "roles", "argument_structure_construction")]
selected_rows$index <- 1:nrow(selected_rows)
return(selected_rows)
}
specific_cxn <- get_utterances_by_arg_struc_cxn(corpus_df, "arg0(np)-arg1(np)-v(v)-arg2(pp)")
View(specific_cxn)
View(specific_cxn[[2]][[2527]])
# Function to extract utterances from data frame that have a specific argument structure construction
get_utterances_by_arg_struc_cxn <- function(data_frame, arg_struc_cxn) {
# Use sapply to check if any of the rows in the arg_struct column of the data frame have the input arg_struc_cxn
# and select the corresponding utterance, roles and arg_struct
selected_rows <- data_frame[sapply(data_frame$argument_structure_construction, function(x) any(x == arg_struc_cxn)), c("utterance", "roles", "argument_structure_construction")]
row.names(selected_rows) <- 1:nrow(selected_rows)
return(selected_rows)
}
specific_cxn <- get_utterances_by_arg_struc_cxn(corpus_df, "arg0(np)-arg1(np)-v(v)-arg2(pp)")
View(specific_cxn)
View(specific_cxn[[2]][[1]])
View(specific_cxn[[2]][[1]])
source("utils.R")
View(freq_table)
coll.analysis()
all_cxn_freq_table <- filter_by_n_most_frequent_arg_struc_cxn(corpus_analysis_df, 50)
write.table(filtered_freq_table, "corpus_analysis_all_cxn.csv", row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
coll.analysis()
write.table(all_cxn_freq_table, "corpus_analysis_all_cxn.csv", row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
coll.analysis()
View(corpus_df)
View(corpus_df[[2]][[1]])
View(roleset_df)
View(roleset_df)
View(roles)
View(freq_table)
# Extract the rolesets from the roles data frame
lemma_df <- data.frame(lemma = unlist(lapply(roles, function(x) x$lemma)))
# Remove NA values
lemma_df <- na.omit(lemma_df)
View(lemma_df)
View(corpus_df)
add_lemma_column(corpus_df, lemma_df)
add_lemma_column <- function(corpus_df, lemma_df) {
corpus_df$lemma <- lemma_df$lemma
return(corpus_df)
}
add_lemma_column(corpus_df, lemma_df)
corpus_df <- add_lemma_column(corpus_df, lemma_df)
View(corpus_df)
count_frequency <- function(data_frame) {
arg_struc_cxn_table <- table(data_frame$arg_struc_cxn)
roleset_table <- table(data_frame$roleset)
lemma_table <- table(data_frame$lemma)
return(list(arg_struc_cxn_table, roleset_table, lemma_table))
}
# Function that creates the dataframe used for the analysis
# It combines the argument structure and roleset into a dataframe
combine_arg_struct_and_roleset <- function(corpus, rolesets, lemmas) {
corpus_analysis_df <- data.frame(arg_struc_cxn = corpus$argument_structure_construction, roleset = rolesets$roleset, lemma = lemmas$lemma)
return(corpus_analysis_df)
}
corpus_analysis_df <- combine_arg_struct_and_roleset(corpus_df, roleset_df, lemma_df)
freq_table <- count_frequency(corpus_analysis_df)
View(freq_table)
View(corpus_analysis_df)
get_utterances_by_arg_struc_cxn(corpus_df, arg0(np)-v(v)-arg1(np)-arg2(adjp))
get_utterances_by_arg_struc_cxn(corpus_df, "arg0(np)-v(v)-arg1(np)-arg2(adjp)")
specific-cxn <- get_utterances_by_arg_struc_cxn(corpus_df, "arg0(np)-v(v)-arg1(np)-arg2(adjp)")
specific_cxn <- get_utterances_by_arg_struc_cxn(corpus_df, "arg0(np)-v(v)-arg1(np)-arg2(adjp)")
View(specific_cxn)
View(specific_cxn)
View(specific_cxn[[2]][[1]])
detect_rolesets_by_lemma <- function(data_frame, lemma) {
# Subset the dataframe to only include rows with the input lemma
lemma_rows <- data_frame[data_frame$lemma == lemma, ]
# Extract the roleset column from the subsetted dataframe
rolesets <- lemma_rows$roleset
# Remove any duplicate rolesets
unique_rolesets <- unique(rolesets)
return(unique_rolesets)
}
corpus_df$rolesets <- apply(corpus_df$lemma, 1, detect_rolesets, data_frame = corpus_df)
detect_rolesets_by_lemma <- function(corpus_df) {
lemma_rolesets_list <- lapply(unique(corpus_df$lemma), function(lemma) {
lemma_rows <- corpus_df[corpus_df$lemma == lemma, ]
lemma_rolesets <- unique(unlist(lemma_rows$roles))
return(list(lemma = lemma, rolesets = lemma_rolesets))
})
lemma_rolesets_df <- do.call(rbind, lemma_rolesets_list)
return(lemma_rolesets_df)
}
lemma_rolesets_df <- detect_rolesets_by_lemma(corpus_analysis_df)
View(lemma_rolesets_df)
View(lemma_rolesets_df)
View(corpus_analysis_df)
View(corpus_analysis_df)
combine_columns <- function(df1, df2, col1, col2) {
# Extract the specified columns from each dataframe
col1_data <- df1[,col1]
col2_data <- df2[,col2]
# Create a new dataframe with the combined columns
combined_df <- data.frame(col1 = col1_data, col2 = col2_data)
# Return the new dataframe
return(combined_df)
}
View(all_cxn_freq_table)
cxn_for_multiple <- filter_by_n_most_frequent_arg_struc_cxn(corpus_analysis_df, 50)
df_lemma_cxn_multiple <- combine_columns(cxn_for_multiple, corpus_analysis_df, "arg_struc_cxn", "lemma")
View(corpus_analysis_df)
View(corpus_analysis_df)
View(corpus_df)
View(corpus_analysis_df)
df_lemma_cxn_multiple <- combine_columns(corpus_analysis_df, corpus_analysis_df, "arg_struc_cxn", "lemma")
View(df_lemma_cxn_multiple)
cxn_for_distinctive <- filter_by_n_most_frequent_arg_struc_cxn(corpus_analysis_df, 2)
View(cxn_for_distinctive)
source("utils.R")
lemma_rolesets_df <- detect_rolesets_by_lemma(corpus_analysis_df)
###freq_table <- count_frequency(corpus_analysis_df)
###cxn_for_multiple <- filter_by_n_most_frequent_arg_struc_cxn(corpus_analysis_df, 50)
df_cxn_lemma_multiple <- combine_columns(corpus_analysis_df, corpus_analysis_df, "arg_struc_cxn", "lemma")
write.table(df_cxn_lemma_multiple, "cxn_lemma_multiple.csv", row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
df_roleset_cxn_multiple <- combine_columns(corpus_analysis_df, corpus_analysis_df, "arg_struc_cxn", "roleset")
write.table(df_cxn_lemma_multiple, "cxn_roleset_multiple.csv", row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
#
df_cxn_roleset_lemma_distinctive <- filter_by_n_most_frequent_arg_struc_cxn(corpus_analysis_df, 2)
df_cxn_lemma_distinctive <- combine_columns(df_cxn_roleset_lemma_distinctive, df_cxn_roleset_lemma_distinctive, "arg_struc_cxn", "lemma")
write.table(df_cxn_lemma_distinctive, "cxn_lemma_distinctive.csv", row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
df_cxn_roleset_distinctive <- combine_columns(df_cxn_roleset_lemma_distinctive, df_cxn_roleset_lemma_distinctive, "arg_struc_cxn", "roleset")
write.table(df_cxn_lemma_distinctive, "cxn_roleset_distinctive.csv", row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
combine_columns <- function(df1, df2, col1, col2) {
# Extract the specified columns from each dataframe
col1_data <- df1[,col1]
col2_data <- df2[,col2]
# Create a new dataframe with the combined columns
combined_df <- data.frame(col1 = col1_data, col2 = col2_data)
colnames(combined_df) <- c(col1, col2)
# Return the new dataframe
return(combined_df)
}
source("utils.R")
lemma_rolesets_df <- detect_rolesets_by_lemma(corpus_analysis_df)
###freq_table <- count_frequency(corpus_analysis_df)
###cxn_for_multiple <- filter_by_n_most_frequent_arg_struc_cxn(corpus_analysis_df, 50)
df_cxn_lemma_multiple <- combine_columns(corpus_analysis_df, corpus_analysis_df, "arg_struc_cxn", "lemma")
write.table(df_cxn_lemma_multiple, "cxn_lemma_multiple.csv", row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
df_roleset_cxn_multiple <- combine_columns(corpus_analysis_df, corpus_analysis_df, "arg_struc_cxn", "roleset")
write.table(df_cxn_lemma_multiple, "cxn_roleset_multiple.csv", row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
#
df_cxn_roleset_lemma_distinctive <- filter_by_n_most_frequent_arg_struc_cxn(corpus_analysis_df, 2)
df_cxn_lemma_distinctive <- combine_columns(df_cxn_roleset_lemma_distinctive, df_cxn_roleset_lemma_distinctive, "arg_struc_cxn", "lemma")
write.table(df_cxn_lemma_distinctive, "cxn_lemma_distinctive.csv", row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
df_cxn_roleset_distinctive <- combine_columns(df_cxn_roleset_lemma_distinctive, df_cxn_roleset_lemma_distinctive, "arg_struc_cxn", "roleset")
write.table(df_cxn_lemma_distinctive, "cxn_roleset_distinctive.csv", row.names = FALSE, sep = "\t", col.names = TRUE, quote = FALSE)
View(coll.analysis)
source("coll.analysis.r")
dist.collexemes()
source("coll.analysis.r")
dist.collexemes <- function(precbitsexponent = precbitsexponent) {
options(warn = -1)
dists <- 1 # remove menu option
input.dc <- 1 # remove menu option
fye.mpfr <- "no" # remove menu option
input.matrix <- read.table(file.choose(), header = TRUE, sep = "\t", quote = "", comment.char = "")
if (input.dc == 1) { # DCA
interim <- t(table(input.matrix))
input.matrix <- data.frame(
as.vector(interim[,1]),
as.vector(interim[,2]), row.names = rownames(interim))
colnames(input.matrix) <- colnames(interim)
} else {
temp <- colnames(input.matrix)[2:3]
input.matrix <- data.frame(
as.vector(input.matrix[,2]),
as.vector(input.matrix[,3]), row.names = input.matrix[,1])
colnames(input.matrix) <- temp
}
construction1.name <- colnames(input.matrix)[1]; construction2.name <- colnames(input.matrix)[2]
# computation
options(warn=-1)
pearson.residuals <- chisq.test(input.matrix, correct=FALSE)$residuals[,1]
all.2.by.2.matrices <- apply(
input.matrix, 1,
\(af) { matrix(c(af, colSums(input.matrix)-af), byrow=TRUE, ncol=2) },
simplify=FALSE)
if (fye.mpfr=="yes") {
FYE.values <- lapply(all.2.by.2.matrices,
\(af) fisher.test.mpfr(af))
}
glms <- lapply(all.2.by.2.matrices,
\(af) glm(rbind(af[1,], af[2,]) ~ c(1:2), family=binomial))
log.odds.ratios <- -sapply(glms, coefficients)[2,]
log.likelihood.values <- sapply(glms, "[[", "null.deviance")
mi.scores <- sapply(all.2.by.2.matrices,
\(af) log2(af[1,1] / chisq.test(af, correct=FALSE)$exp[1,1]))
delta.p.constr.cues.word <- sapply(all.2.by.2.matrices,
\(af) af[1,1]/sum(af[,1]) - af[1,2]/sum(af[,2]))
delta.p.word.cues.constr <- sapply(all.2.by.2.matrices,
\(af) af[1,1]/sum(af[1,]) - af[2,1]/sum(af[2,]))
relations <- sapply(pearson.residuals,
\(af) switch(sign(af)+2, construction2.name, "chance", construction1.name))
# output
output.table <- data.frame(WORD=rownames(input.matrix), CONSTRUCTION1=input.matrix[,1], CONSTRUCTION2=input.matrix[,2], row.names=NULL)
output.table <- data.frame(output.table, PREFERENCE=relations, LLR=log.likelihood.values, PEARSONRESID=pearson.residuals,
LOGODDSRATIO=log.odds.ratios, MI=mi.scores,
DELTAPC2W=delta.p.constr.cues.word, DELTAPW2C=delta.p.word.cues.constr, row.names=NULL)
if (fye.mpfr=="yes") {
output.table <- data.frame(output.table,
# FYE=sapply(FYE.values, formatMpfr, digits=12))
FYE=sapply(sapply(FYE.values, \(af) -log10(af)), asNumeric))
}
colnames(output.table)[2:3] <- c(construction1.name, construction2.name)
output.table <- output.table[order(output.table$PREFERENCE, -output.table$LOGODDSRATIO),]
write.table(output.table, file=save.date <- gsub(":", "-", paste0(Sys.time(), ".csv")), sep="\t", row.names=FALSE, col.names=TRUE, quote=FALSE)
cat("\n\nThe results are in the file called ", save.date, ".\n")
plot(log2(output.table[,2]+output.table[,3]), output.table$LOGODDSRATIO, type="n",
xlab="Logged co-occurrence frequency", ylab="Association (log odds ratio)")
grid(); abline(h=0, lty=2); abline(v=0, lty=2)
text(log2(output.table[,2]+output.table[,3]), output.table$LOGODDSRATIO, output.table$WORD, font=3)
}
dist.collexemes()
