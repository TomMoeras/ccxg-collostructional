% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\KOMAoption{captions}{tableheading}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Computational Construction Grammar and Collostructional Analysis on a PropBank-annotated Corpus},
  pdfauthor={Thomas Moerman},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Computational Construction Grammar and Collostructional Analysis
on a PropBank-annotated Corpus}
\author{Thomas Moerman}
\date{}

\begin{document}
\maketitle
\begin{abstract}
This paper uses Computational Construction Grammar approach and
collostructional analysis on a PropBank-annotated corpus. It was written
during an internship at the VUB AI-Lab. The corpus and dataset were
accessed through this internship.
\end{abstract}
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[boxrule=0pt, frame hidden, interior hidden, enhanced, borderline west={3pt}{0pt}{shadecolor}, breakable, sharp corners]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{3}
\tableofcontents
}
\listoffigures
\listoftables
\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Construction Grammar (CxG) is a general term for a family of theories
used for describing and interpreting language. It emphasises the
importance of regular patterns of language use, known as constructions.
It argues that these constructions capture all linguistic knowledge in
the shape of form-meaning pairs. In other words, in CxG the traditional
concepts of lexicon and grammar are combined into one. There is no
distinction between the two as form is inherently connected to meaning
(\textbf{allthecxgboysandgirls?}). Collostructional analysis, developed
by (\textbf{griesstefall?}), is a combination of several quantative
(statistical) methods for examining the relationship between words
(lemmas) and structures. The term collostruction, a blend of the words
collocation and construction, describes the method of measuring the
level of attraction or repulsion that words have towards specific
syntactic constructions.

Most studies using collostructional analysis have focused on the
relationship between verbs and constructions that convey information
about argument structure ((\textbf{gries2013?})). This present paper
follows in those footsteps but will take a slightly different approach.
It still studies the patterns of co-occurrence between verbs and
specific English argument constructions. However, instead of only
examining the lemma, this paper focuses on the role-set or word sense of
a specific lemma in relation to its place in an argument structure
construction. It does this by performing collustructional analysis
(\textbf{specifywhichones?}) on a data set taken from the
propbank-annotated OntoNotes 5.0 corpus. What follows is a brief
description of the corpus and data set used.

OntoNotes 5.0 (\textbf{Weischedel2013?}) is a corpus consisting of a
subset of English, Chinese and Arabic texts. It can be described as a
broad-coverage corpus as it spans several genres including religious
texts, telephone conversations, news articles and weblogs. In total it
consists of 2.9 million words. The corpus is annotated with a number of
different layers of information. These are presented in the corpus as
follows:

\hypertarget{sample-paper_filesfigure-htmlexample-ontonotes-annotation.png-vaneeckeppt}{%
\section{\texorpdfstring{{[}{]}\#(sample-paper\_files/figure-html/example-ontonotes-annotation.png/)
(\textbf{VanEeckePPT?})}{{[}{]}\#(sample-paper\_files/figure-html/example-ontonotes-annotation.png/) (VanEeckePPT?)}}\label{sample-paper_filesfigure-htmlexample-ontonotes-annotation.png-vaneeckeppt}}

In the above example, the utterance ``With their unique charm, these
well-known cartoon images once again caused Hong Kong to be a focus of
worldwide attention.'' is annotated with a treebank, propbank, word
sense annotation, ontology, coreference and entity names annotation
layer. A complete description of these annotations can be found in
(\textbf{Xue2012?}). For the purpose of the present study, the propbank
annotation layer is of particular interest. The propbank layer
(https://propbank.github.io/) annotates the argument structure of verbs.
It does this by providing a list of possible roles for each verb. These
roles are called arguments. The word sense annotation layer provides a
list of possible word senses for each word. These word senses are also
called role sets. The propbank layer is used to identify the argument
structure. The word sense layer is used to identify the specific word
sense of a verb. To illustrate how these annotations should be
interpreted, consider the following utterance:

\begin{itemize}
\tightlist
\item
  She gave Peter a watch.
\end{itemize}

In this utterance, the verb ``gave'' would be annotated with the lemma
``give'' and the role set ``give.01'' which is described as ``transfer''
in https://propbank.github.io/v3.4.0/frames/. The verb here is the frame
evoking element (FEE). The role set ``give.01'' has three arguments:
``Arg0'', ``Arg1'' and ``Arg2''. In propbank annotation this means that
``Arg0'' is the giver, ``Arg1'' is the thing given and ``Arg2'' is the
entity given to. In traditional grammar, these arguments would be
described as the agent (Arg0), patient (Arg1) and
instrument/benefactive/attribute (Arg2). This paper will use the
propbank terminology. This means that the above utterance would be
annotated as follows: ``She(Arg0-NP) gave(FEE-V) Peter(Arg2-NP) a
watch(Arg1-NP)''. In other words, the argument structure construction in
which the roleset ``give.01'' is embedded is ``Arg0-NP FEE-V Arg2-NP
Arg1-NP''. This can be recognised as a ditransitive construction. This
paper focusses broadly on ditransitive constructions in its various
manifestations.

The data set used for this analysis is a subset of the OntoNotes 5.0
corpus. This data set was extracted from the corpus using the
(\textbf{CCxG?}) Explorer (\textbf{VanEecke2018?}). The CCxG Explorer is
a tool developed by the Evolutionary \& Hybrid AI (EHAI) research team
at the VUB Artificial Intelligence Lab. Its goal was twofold. From a
broad-coverage corpus, it wanted to, first, gain linguistic insights
from large-scale construction grammar analyses regarding the English
argument structure and, secondly, show the application potential of CCxG
by operationalising it on a large scale (Beuls \& Van Eecke, 2021). As a
result, the CCxG Explorer allows usage-based linguists to search for
corpus examples that match a particular semantic structure. This is
useful because it provides the option to find examples of
morphosyntactic phenomena without the need to identify them explicitly
((\textbf{Beuls?}) \& Van Eecke, submitted). The CCxG Explorer can be
accessed on used on the web at https://ehai.ai.vub.ac.be/ccxg-explorer/.
Its source code is publically available on Gitlab as part of the babel
toolkit (\textbf{babelrepo?}). The method and accuracy of the CCxG
Explorer has not yet been established. This is currently being
investigated. This is why this present paper takes a cautious approach
to the results.

To extract the ditransitive constructions, the following schema was used
in the CCxG Explorer: ``Arg0-NP FEE-V Arg1-NP Arg2''. The precise order
is not taken into account which means that, for example, the following
constructions are also extracted: ``Arg0-NP FEE-V Arg2-NP Arg1'',
``Arg1-NP FEE-V Arg0-NP Arg2'' and ``Arg1-NP FEE-V Arg2-NP Arg0''. The
reason for this is that the order of the arguments is not fixed in
ditransitive constructions and this schema allows for a more broad
exploration. Further, it should be noted that there is no part of speech
specified in the Arg2 slot. This is because the Arg2 slot can be filled
by not only a NP but also, for example, a PP which would result in the
dative alternation of the ditransitive construction.

The search results for this particular schema contained 9339 utterances.
They were downloaded as a .json file. This .json file was the raw data
for this analysis. It contains the string of the utterances and the
roles that are defined in that utterance. The roles are further
specified by roleType, part of speech (pos), string, indices, role set
and lemma. In those 9339 utterances, there were 844 unique lemmas, 924
unique rolesets and 87 unique argument structure constructions which
appeared in the previously mentioned schema.

It is not this study's aim to come to any definitive conclusions about
the relationship between word sense and argument structure
constructions. Instead, its goal is explore the possibilities of using
propbank-annotated corpora and Computational Construction Grammar (CCxG)
in a collostructional analysis.

This paper consists of several sections. First, a short overview of the
(theoretical) background regarding the CCxG Explorer is provided. In
addition, the research methodology is presented, specifically on how the
data set was analysed. This is followed by a presentation, analysis and
discussion of the relevant data. The analysis section consists of two
parts. First, the cleaned data is examined and second, the results from
the Distinctive Collexeme Analysis (DCA) are presented and discussed.

\hypertarget{data-processing-and-methodology}{%
\section{Data Processing and
Methodology}\label{data-processing-and-methodology}}

The raw data was processed in R using the `jsonlite' package
(\textbf{jsonlite?}). The data was then converted to a data frame and
the relevant columns were selected. The raw data file does not contain
the full argument structure construction in one string. To identify and
classify the argument structure construction from each utterance a
function was used. The resulting data frame contains the argument
structure construction, role set and lemma. This data frame contains all
the data used for the analysis. This is shown in
Table~\ref{tbl-raw-corpus-data}.

\hypertarget{tbl-raw-corpus-data}{}
\begin{table}
\caption{\label{tbl-raw-corpus-data}Raw corpus data used for the subsequent analyses (first 10 rows) }\tabularnewline

\centering
\begin{tabular}[t]{l|l|l}
\hline
arg\_struc\_cxn & roleset & lemma\\
\hline
arg0(np)-v(v)-arg1(np)-arg2(c("s", "vp")) & invite.01 & invite\\
\hline
arg0(np)-v(v)-arg1(np)-arg2(pp) & force.01 & force\\
\hline
arg0(np)-v(v)-arg1(np)-arg2(pp) & connect.01 & connect\\
\hline
arg0(np)-v(v)-arg1(np)-arg2(pp) & receive.01 & receive\\
\hline
arg0(np)-v(v)-arg1(np)-arg2(pp) & draw.02 & draw\\
\hline
arg0(np)-v(v)-arg1(np)-arg2(pp) & lay.01 & lay\\
\hline
arg0(np)-v(v)-arg1(np)-arg2(c("s", "vp")) & order.01 & order\\
\hline
arg0(np)-v(v)-arg1(np)-arg2(np) & call.01 & call\\
\hline
arg0(np)-v(v)-arg1(np)-arg2(pp) & deal.02 & deal\\
\hline
arg0(np)-v(v)-arg1(np)-arg2(adjp) & turn.02 & turn\\
\hline
\end{tabular}
\end{table}

Table~\ref{tbl-raw-corpus-data} shows a small sample of data used for
subsequent analyses. The table contains three columns: arg\_struc\_cxn,
roleset, and lemma. The arg\_struc\_cxn column lists the argument
structure construction that the roleset occurs in. The roleset column
lists the specific roleset that the lemma takes on. The lemma column
lists the lemma of the verb.

As previously mentioned, the type of analysis used for this paper is a
collustructrional analysis. According to Stefanowitsch
(\textbf{Steffi2013?}), there are several types of collustructrional
analysis. There is a Simple Collexeme Analysis (SCA), Distinctive
Collexeme Analysis (DCA) and Covarying Collexeme Analysis (CCA). The
type of analysis used for this paper is the distinctive collexeme
analysis. Since this is the only relevant type of analysis for this
paper, the other types of analysis will not be discussed further.

DCA (\textbf{GriesStef2004a?}) compares all words that occur in a slot
of two similar constructions. It is based on the frequency of the word
and constructions it occurs in. (\textbf{StefGries2013?}) presents the
following Table~\ref{tbl-dca} to illustrate which frequency information
is needed for a DCA.

\hypertarget{tbl-dca}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2121}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2803}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2879}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2197}}@{}}
\caption{\label{tbl-dca}Frequency information needed for a distinctive
collexeme analysis}\tabularnewline
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\textbf{Word l of Class L}
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\textbf{Other Words of Class L}
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\textbf{Total}
\end{minipage} \\
\midrule()
\endfirsthead
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\textbf{Word l of Class L}
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\textbf{Other Words of Class L}
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\textbf{Total}
\end{minipage} \\
\midrule()
\endhead
Construction c1 of Class C & Frequency of L(l) in C(c1) & Frequency of
L(-l) in C(c1) & Total frequency of C(c1) \\
Construction c2 of Class C & Frequency of L(l) in C(c2) & Frequency of
L(-l) in C(c2) & Total frequency of C(c2) \\
Total & Total frequency of L(l) in C(c1,c2) & Total frequency of L(-l)
in C(c1,c2) & Total frequency of C(c1,c2) \\
\bottomrule()
\end{longtable}

In this table, the frequency of a word (l \& -l) and construction (c1 \&
c2) are mapped to each other to create a contingency table containing
the frequency of l in c1, l in c2, -l in c1, -l in c2. These are then
combined to create a total frequency. Such a contingency table can then
be used to perform a contingency test to return association measures
like, for example, the Fisher-Yates p score. The Fisher-Yates p score is
a measure of the strength of the association between a word and a
construction. the p-value represents the probability of obtaining a test
statistic as extreme or more extreme than the one observed. A small
p-value (typically less than 0.05) suggests that the null hypothesis can
be rejected, and that the difference in co-occurrence frequencies
between the two words being compared is not due to chance. Conversely, a
large p-value (typically greater than 0.05) suggests that there is not
enough evidence to reject the null hypothesis and that the difference in
co-occurrence frequencies may be due to chance. In other words, a small
p-value means that the difference in co-occurrence frequencies between
the two words is statistically significant and it is unlikely that the
observed difference is due to chance Levshina (2015). It is important to
note that in DCA there is a focus on the differences between
constructions. To uncover similarities between construction a SCA can be
used (\textbf{GriesStef2004a?}) but this is not done here given the
limited scope of the present research.

An extension of DCA is the possibility to perform it on a data set that
has more than two types of constructions. This is referred to as
Multiple Distinctive Collexeme Analysis (MDCA)
(\textbf{StefGries2004a?}). MDCA is a method that can be used to compare
the distinctive collexemes of multiple constructions. It is based on the
same principles as DCA, but instead of comparing two constructions, it
compares multiple constructions. In order to perform a MDCA on a data
set a multidimensional contingency table is required.
(\textbf{GriesStef2013?}) gives the following Table~\ref{tbl-mca} to
illustrate which frequency information is needed for a MDCA.:

\hypertarget{tbl-mca}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1867}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2867}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2933}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2333}}@{}}
\caption{\label{tbl-mca}Frequency information needed for a multiple
distinctive collexeme analysis}\tabularnewline
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\textbf{Word l of Class L}
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\textbf{Other Words of Class L}
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\textbf{Total}
\end{minipage} \\
\midrule()
\endfirsthead
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\textbf{Word l of Class L}
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\textbf{Other Words of Class L}
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\textbf{Total}
\end{minipage} \\
\midrule()
\endhead
Construction c1 of Class C & Frequency of L(l) in C(c1) & Frequency of
L(-l) in C(c1) & Total frequency of C(c1) \\
Construction c2 of Class C & Frequency of L(l) in C(c2) & Frequency of
L(-l) in C(c2) & Total frequency of C(c2) \\
\ldots{} & \ldots{} & \ldots{} & \ldots{} \\
Construction c(n) & Freq. of L(l) in C(cn) & Freq. of L(-l) in C(cn) &
Total frequency of C(cn) \\
Total & Total frequency of L(l) in C(c1,c2, \ldots n) & Total frequency
of L(-l) in C(c1,c2, \ldots n) & Total frequency of C(c1,c2,
\ldots n) \\
\bottomrule()
\end{longtable}

As can be observed in the above table, the difference is that in the
MDCA contingency table there is an n number of constructions represented
in the columns. Due to the limited scope of this paper, only the DCA
will be used. However, the MDCA was still performed and the results are
available in the appendix because it could prove useful for future
research.

As already discussed, both the DCA and MDCA are used on the data set in
this paper but only the DCA will be presented and discussed. The DCA is
used to compare the distinctive collexemes of the two most frequent
constructions in the data set. The MDCA can be used to compare the
distinctive collexemes of all constructions in the data set. To perform
both types of analyses, a R script developed by
(\textbf{GriesStef2013?}) was used and adapted to suit the data set in
this paper. This script calculates the association measures for all
words (lemma / role set) in relation to the two most frequent argument
structure constructions from the data set and returns a table containing
the association measures for each of those word. In total, eightDE
association measures are given. They are presented in
Table~\ref{tbl-accosm}. Not all of them are used in the analysis but
they are presented for completeness and to give a general overview of
the measures that are available when using DCA on a PropBank-annotated
corpus. A comparison between these measures is not examined in this
paper.

\hypertarget{tbl-accosm}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1321}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3208}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5472}}@{}}
\caption{\label{tbl-accosm}Association measures used in the
DCA}\tabularnewline
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Association Measure
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Full Name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Score Indication
\end{minipage} \\
\midrule()
\endfirsthead
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Association Measure
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Full Name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Score Indication
\end{minipage} \\
\midrule()
\endhead
LLR & Log-Likelihood Ratio & The higher the value, the stronger the
association between the two variables. \\
PEARSONRESID & Pearson Residual & The closer the value is to 0, the
stronger the association between the two variables. \\
LOGODDSRATIO & Log Odds Ratio & The higher the value, the stronger the
association between the two variables. \\
MI & Mutual Information & The higher the value, the stronger the
association between the two variables. \\
DELTAPC2W & Difference in Probability of Construction to Word & The
higher the value, the stronger the association between the two
variables. \\
DELTAPW2C & Difference in Probability of Word to Construction & The
higher the value, the stronger the association between the two
variables. \\
FYE & Fisher-Yates Exact Test & The lower the value, the stronger the
association between the two variables. \\
\bottomrule()
\end{longtable}

\hypertarget{results-and-discussion}{%
\section{Results and Discussion}\label{results-and-discussion}}

In the following section, an exploratory analysis will be conducted on
the cleaned corpus data and on the resulting data from the DCA. The
analysis departs from an overview of the frequency of the lemmas and
rolesets in specific argument constructions. In
Table~\ref{tbl-alph-summary}, a random subset of an alphabetically
sorted list is shown. The argument structure constructions are ordered
based on the frequency in which they appear in the data set.

\hypertarget{tbl-alph-summary}{}
\begin{table}
\caption{\label{tbl-alph-summary}Alphabetical sorted summary of the data set (random 20 rows and 8
columns). }\tabularnewline

\centering
\begin{tabular}[t]{llrrrrrr}
\toprule
lemma & roleset & total & arg0(np)-v(v)-arg1(np)-arg2(pp) & arg0(np)-v(v)-arg2(np)-arg1(np) & arg0(np)-v(v)-arg1(np)-arg2(c("s", "vp")) & arg0(np)-v(v)-arg1(np)-arg2(vp) & arg0(np)-v(v)-arg1(np)-arg2(np)\\
\midrule
cast & cast.02 & 2 & 2 &  &  &  & \\
castigate & castigate.01 & 3 & 3 &  &  &  & \\
catapult & catapult.01 & 1 & 1 &  &  &  & \\
catch & catch.02 & 5 & 1 &  &  & 3 & \\
catch & catch.03 & 3 & 3 &  &  &  & \\
\addlinespace
cede & cede.01 & 2 & 2 &  &  &  & \\
challenge & challenge.01 & 5 & 3 &  & 2 &  & \\
change & change.01 & 22 & 21 &  & 1 &  & \\
channel & channel.01 & 2 & 2 &  &  &  & \\
characterize & characterize.01 & 8 & 6 &  &  &  & \\
\addlinespace
charge & charge.01 & 3 & 1 & 2 &  &  & \\
charge & charge.04 & 1 & 1 &  &  &  & \\
charge & charge.05 & 21 & 21 &  &  &  & \\
charter & charter.01 & 2 &  &  & 1 &  & \\
chastise & chastise.01 & 3 & 3 &  &  &  & \\
\addlinespace
chide & chide.01 & 2 & 2 &  &  &  & \\
choose & choose.01 & 3 & 1 &  &  &  & \\
cite & cite.01 & 12 & 11 &  &  &  & \\
cite & cite.02 & 1 & 1 &  &  &  & \\
claim & claim.01 & 1 & 1 &  &  &  & \\
\addlinespace
clean & clean.01 & 2 & 2 &  &  &  & \\
\bottomrule
\end{tabular}
\end{table}

What Table~\ref{tbl-alph-summary} shows is that a lemma manifests itself
in different rolesets in different constructions. For example, the lemma
``catch'' appears in role sets ``catch.02'' and ``catch.03'' and the
lemma ``charge'' appears in the role sets ``charge.01'', ``charge.04''
and ``charge.05''. \textbf{?@tbl-sense-utterance} illustrates these role
sets with their semantic meaning and an example utterance from the
corpus.

\begin{description}
\item[-\textgreater{} insert table here]
Example of the role sets and their semantic meaning
\{\#tbl-sense-utterance\}
\end{description}

-\textgreater{} insert table explanation here

The next Table~\ref{tbl-freq-summary} shows a similar list but now no
longer alphabetically sorted but sorted based on the frequency of the
lemmas and rolesets in the data set. The argument structure
constructions are still ordered based on the frequency in which they
appear in the data set.

\hypertarget{tbl-freq-summary}{}
\begin{table}
\caption{\label{tbl-freq-summary}Frequency ordered overview of the data set (first 10 rows and 8
columns). }\tabularnewline

\centering
\begin{tabular}[t]{llrrrrrr}
\toprule
lemma & roleset & total & arg0(np)-v(v)-arg1(np)-arg2(pp) & arg0(np)-v(v)-arg2(np)-arg1(np) & arg0(np)-v(v)-arg1(np)-arg2(c("s", "vp")) & arg0(np)-v(v)-arg1(np)-arg2(vp) & arg0(np)-v(v)-arg1(np)-arg2(np)\\
\midrule
give & give.01 & 976 & 224 & 662 &  &  & 1\\
put & put.01 & 443 & 355 &  &  &  & 2\\
use & use.01 & 381 & 76 &  & 284 &  & \\
bring & bring.01 & 243 & 175 & 27 &  &  & 4\\
take & take.01 & 225 & 189 &  & 3 & 1 & 1\\
\addlinespace
call & call.01 & 195 & 3 &  &  & 2 & 130\\
tell & tell.01 & 192 & 19 & 150 &  &  & \\
send & send.01 & 188 & 111 & 49 &  & 1 & 3\\
spend & spend.02 & 134 & 49 &  & 74 & 2 & \\
force & force.01 & 118 & 22 &  & 2 & 80 & \\
\bottomrule
\end{tabular}
\end{table}

The analysis will depart from the DCA lemma \& role set table, which
contains all calculated association scores. The analysis starts with
giving a general overview of the most frequent and distinctive
collostructions found in the corpus. Then, it will proceed to examine
the specific association measures in order to gain insight into the
patterns and preferences in the data. Examples of the collostructions
will be provided to assist in understanding the findings and
interpreting the results. The full results of the analyses are available
in the appendix. The results of the DCA are presented in
Table~\ref{tbl-dca-roleset}:

\hypertarget{tbl-dca-roleset}{}
\begin{table}
\caption{\label{tbl-dca-roleset}Results DCA analysis. }\tabularnewline

\centering
\begin{tabular}[t]{l|r|r|l|r|r|r|r|r|r|r}
\hline
WORD & arg0(np)-v(v)-arg1(np)-arg2(pp) & arg0(np)-v(v)-arg2(np)-arg1(np) & PREFERENCE & LLR & PEARSONRESID & LOGODDSRATIO & MI & DELTAPC2W & DELTAPW2C & FYE\\
\hline
put.01 & 355 & 0 & arg0(np)-v(v)-arg1(np)-arg2(pp) & 142 & 3.7 & 27 & 0.28 & 0.06 & 0.19 & 16.0\\
\hline
take.01 & 189 & 0 & arg0(np)-v(v)-arg1(np)-arg2(pp) & 75 & 2.7 & 26 & 0.28 & 0.03 & 0.18 & 8.6\\
\hline
accuse.01 & 100 & 0 & arg0(np)-v(v)-arg1(np)-arg2(pp) & 39 & 1.9 & 26 & 0.28 & 0.02 & 0.18 & 5.1\\
\hline
use.01 & 76 & 0 & arg0(np)-v(v)-arg1(np)-arg2(pp) & 30 & 1.7 & 26 & 0.28 & 0.01 & 0.18 & 3.9\\
\hline
receive.01 & 75 & 0 & arg0(np)-v(v)-arg1(np)-arg2(pp) & 29 & 1.7 & 26 & 0.28 & 0.01 & 0.18 & 4.0\\
\hline
keep.04 & 73 & 0 & arg0(np)-v(v)-arg1(np)-arg2(pp) & 29 & 1.7 & 25 & 0.28 & 0.01 & 0.18 & 3.8\\
\hline
get.01 & 71 & 0 & arg0(np)-v(v)-arg1(np)-arg2(pp) & 28 & 1.6 & 25 & 0.28 & 0.01 & 0.18 & 4.0\\
\hline
throw.01 & 68 & 0 & arg0(np)-v(v)-arg1(np)-arg2(pp) & 27 & 1.6 & 25 & 0.28 & 0.01 & 0.18 & 3.9\\
\hline
thank.01 & 57 & 0 & arg0(np)-v(v)-arg1(np)-arg2(pp) & 22 & 1.5 & 25 & 0.28 & 0.01 & 0.18 & 3.4\\
\hline
describe.01 & 52 & 0 & arg0(np)-v(v)-arg1(np)-arg2(pp) & 20 & 1.4 & 25 & 0.28 & 0.01 & 0.18 & 3.0\\
\hline
\end{tabular}
\end{table}

In these tables, the first column ``WORD'' shows the lemma / roleset of
the word. The next two columns show the frequency of the word in two
different constructions: ``arg0(np)-v(v)-arg1(np)-arg2(pp)'' and
``arg0(np)-v(v)-arg2(np)-arg1(np)''. The ``PREFERENCE'' column shows the
construction that the word is more frequent in. The next columns
``LLR'', ``PEARSONRESID'', ``LOGODDSRATIO'', ``MI'', ``DELTAPC2W'',
``DELTAPW2C'' and ``FYE'' are association measures that gives
information on the strength of the association between the word and the
construction it appears in. The LLR is likelihood ratio test statistics,
PEARSONRESID is the Pearson residual, LOGODDSRATIO is log odds ratio, MI
is mutual information, DELTAPC2W is delta probability of collocate to
word, DELTAPW2C is delta probability of word to collocate, and FYE is
the Fisher-Yates p value. The values of these measures are used to
compare the strength of association between the word and the
construction it appears in.

A first observation that can be made is that the two most frequent
argument structure constructions and were thus used for the DCA are the
arg0(np)-v(v)-arg1(np)-arg2(pp) and arg0(np)-v(v)-arg2(np)-arg1(np).
These could, in more traditional terms, be described as the
propositional dative and ditransitive construction. Additionally, there
are three categories on how the role sets / lemmas are related to the
two constructions. They can appear exclusively in either
arg0(np)-v(v)-arg1(np)-arg2(pp) or arg0(np)-v(v)-arg2(np)-arg1(np), or
they appear in both.

To interpret the results of the DCA they are compared to a study by
(\textbf{GriesStef2004a?}) where a DCA was used to compare collexemes in
a ditransitive and to-dative construction. The results of this study are
presented in the following table:

------------ results table stef gries 2004

discussion DCA

The results of the MDCA are presented in the following table. It shows

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset }\OtherTok{\textless{}{-}} \FunctionTok{read\_tsv}\NormalTok{(}\FunctionTok{here}\NormalTok{(}\StringTok{"register{-}analysis.tsv"}\NormalTok{), }\AttributeTok{show\_col\_types =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{fa }\OtherTok{\textless{}{-}}\NormalTok{ dataset }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{row.names =} \StringTok{"filename"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as.matrix}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{factanal}\NormalTok{(}\AttributeTok{factors =} \DecValTok{4}\NormalTok{, }\AttributeTok{scores =} \StringTok{"regression"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To interpret the results of the MCA

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

The conclusion should recap the paper, summarizing what was said in each
section and how everything ties together. It might feel redundant, in
particular in relation to the introduction, but that's precisely the
point: for someone who has not heard your ideas, redundancy is key to
understand. Be clear about how each section contributes to your main
point and what the take-home message is.

Of course, the language of the paper should be formal and academic (I
appreciate puns and jokes, but don't lower the register too much).
Coherence in the ideas, cohesion between the sentences and appropriate
use of the technical vocabulary and of connectors (e.g.~\emph{however},
\emph{in contrast}, \emph{while}\ldots) are important and \textbf{will
be evaluated}. Not because of ``language'' evaluation but because these
aspects are crucial for understanding, and if the reader needs to read a
sentence many times and/or have previous knowledge of your process in
order to understand the text, it's not well written. I also recommend
checking out the \texttt{\{spelling\}} package to run some spelling
checks on your files!

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-levshina_2015}{}}%
Levshina, Natalia. 2015. \emph{How to do linguistics with {R}: Data
exploration and statistical analysis}. {Amsterdam; Philadelphia}: {John
Benjamins Publishing Company}.

\end{CSLReferences}



\end{document}
